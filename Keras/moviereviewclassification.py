# -*- coding: utf-8 -*-
"""MovieReviewClassification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1FZBZKvFCytfW2FWvf3ZZamVcVVR1AVaf
"""

# loading dataset

from keras.datasets import imdb 

(training_data, training_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)

len(training_data[0])

# find the maximum value contained in the training_data[i] arrays

max([max(sequence) for sequence in training_data ]) #sequence is a python list

idx_word = imdb.get_word_index()
# so this will give me an index, given a word, 
# now I will find a reverse map, which will give me a word given an index

reverse_dic = { index: word for word, index in idx_word.items()  }
#print(reverse_dic)
oneReview = [ reverse_dic.get(i-3, '?') for i in training_data[0]]
oneReview = ' '.join(word for word in oneReview)
print(oneReview)
#training_data[0]
#reverse_dic[1]

import numpy as np
# vectorizing the one hot encoding embeddings

def vectorizeThis(data, dimension = 10000):
  # initializing, with shape (number of examples, words_per_example)
  vectorized = np.zeros((data.shape[0], dimension))
  # perhaps only allowed for numpy arrays? Not sure. could be. 
  for i, sequence in enumerate(data):
    vectorized[i, sequence] = 1 
  return vectorized

training_data = vectorizeThis(training_data)
test_data = vectorizeThis(test_data)
# data has been vectorized
print(training_labels)
training_labels = np.asarray(training_labels).astype('float32')
test_labels = np.asarray(test_labels).astype('float32')
print(training_labels)

# I can prepare a model I guess, right? dense layers?
# let's have an attempt before I look

from keras import models
from keras import layers

# a simple two layer nn architecture

nn = models.Sequential()
nn.add(layers.Dense( 16, activation = 'relu', input_shape= (10000,)))
nn.add(layers.Dense(16, activation = 'relu'))
nn.add(layers.Dense(1, activation= 'sigmoid'))

# now compile the network 
# with-
# optimizer
#loss function
# learning_rate
# metrics  

print(training_data.shape)
nn.compile(optimizer ='rmsprop', loss='binary_crossentropy', metrics = ['accuracy'])

partial_training_data = training_data[:10000]
validation_training_data = training_data[10000:]

partial_training_labels = training_labels[:10000]
validation_training_labels = training_labels[10000:]

history = nn.fit(partial_training_data, partial_training_labels, batch_size = 128, epochs=20, validation_data=(validation_training_data, validation_training_labels))

#loss , test_acc = nn.evaluate(test_data, test_labels)

test_acc

history_dict = history.history
history_dict.keys()

# let's try stuff with matplot lib 

import matplotlib.pyplot as plt

# these are the lists with the values to the plotted
acc = history_dict['acc']
val_acc = history_dict['val_acc']
loss = history_dict['loss']
val_loss = history_dict['val_loss']

iterations = range(1, len(acc) + 1)

# plt.plot(X, Y, colorCode,label = "label")
plt.plot( iterations, loss, 'bo', label = "Training loss" )
plt.plot(iterations, val_loss, 'b', label = 'Validation loss')
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.legend()
plt.title("Training vs validation loss")
plt.show()

plt.clf()

plt.plot(iterations, acc, label= "Training accuracy")
plt.plot(iterations, val_acc, label= "Validation accuracy")
plt.xlabel("Epochs")
plt.ylabel("Accuracy")
plt.legend()
plt.show()

