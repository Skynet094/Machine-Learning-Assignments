# -*- coding: utf-8 -*-
"""Hello_DL.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1YNYJjtOHi5gGZO18Q-yZsVLOuZk5uDr-
"""

from keras.datasets import mnist

# Data loading part

(train_data, train_labels), (test_data, test_labels) = mnist.load_data()

from keras.utils import to_categorical

# Preprocessing data 
# making the data into column vectors, from 2D gray scale images 

# Same block e duibar code run korle notun kore init hocchena, karon data age jaygay load kora, so
# ager data er upore reshape cholbe, error khabe
# be careful!

print(train_data.shape)
print(test_data.shape)
train_data = train_data.reshape((60000, 28, 28))
test_data = test_data.reshape((10000, 28, 28))
train_data = train_data[: , 7:-7, 7:-7]
test_data = test_data[ : , 7:-7, 7:-7]

train_data = train_data.reshape((60000, 14*14))
test_data = test_data.reshape((10000, 14*14))


train_data = train_data.astype('float32')/255
test_data = test_data.astype('float32')/255

train_labels = to_categorical(train_labels)
test_labels = to_categorical(test_labels)

print(train_data.shape)
print(test_data.shape)

from keras import models
from keras import layers
# prepare the Model architecture 
# for each layer, we got some hyper parameters
# number of neurons, (the first layer for input size)
# activation function

nn = models.Sequential()
nn.add( layers.Dense(128 , activation='relu', input_shape= (14* 14,)))
nn.add(layers.Dense(10, activation='softmax'))

# now , we set another set of hyper parameters
# the loss function
# the optimizer
# I guess learning rate should be one,  
# then choice of regularlizer 
# then..
# metrics we wanna compute

nn.compile( optimizer='adam', loss='categorical_crossentropy', metrics = ['accuracy'])

# now we fit the data, aka , now we feed the data to the network, to the distillation process and then
# here, we set the batch number, epochs 

nn.fit(train_data, train_labels, epochs = 10, batch_size= 64)
# don't be too happy seeing the accuracy here, for it is but training data!!!!!!!!!!!!!!

# here, we evaluate with test / unseen data 

test_loss, test_accuracy = nn.evaluate(test_data, test_labels)

# here be aware of the accuracy and other metrics and stuff. 
print( test_accuracy * 100)

